{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee58206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import all required models\n",
    "from sklearn.linear_model import LinearRegression, Ridge \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, StackingRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26cbf74",
   "metadata": {},
   "source": [
    "# Summary of the Story \n",
    "\n",
    "- Part A: My baseline DecisionTree was too simple (high bias) and underfit the data.\n",
    "\n",
    "\n",
    "- Part B (Bagging): I tried to fix it with Bagging, but it failed. This proves Bagging is for variance reduction, not bias reduction.\n",
    "- Part B (Boosting): I then tried Boosting, which is a bias-reduction technique, and it worked spectacularly, cutting the error in half.\n",
    "\n",
    "\n",
    "- Part C (Stacking): My Stacking model performed almost identically to the Boosting model, as its other components were too weak to add value, proving that a stack is often only as strong as its best base learner.\n",
    "\n",
    "\n",
    "- Final (Part D): Therefore, the Gradient Boosting Regressor was the best-performing model, as this problem was one of high bias, not high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600af80",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163116d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "\n",
    "# Load the hourly dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"hour.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'hour.csv' not found. Please ensure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Feature Engineering per assignment\n",
    "df = df.drop(columns=['instant', 'dteday', 'casual', 'registered'])\n",
    "TARGET = 'cnt'\n",
    "categorical_features = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c6e03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 13903\n",
      "Test set size: 3476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Time-Series Train-Test Split ---\n",
    "test_size = 0.2\n",
    "test_split_index = int(len(X) * (1 - test_size))\n",
    "\n",
    "X_train = X.iloc[:test_split_index]\n",
    "y_train = y.iloc[:test_split_index]\n",
    "X_test = X.iloc[test_split_index:]\n",
    "y_test = y.iloc[test_split_index:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7e60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Create Preprocessing Pipeline ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# preform one hot encoding on categorical features\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e9bfa",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c112465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part A: Baseline Models ---\n",
      "Baseline Linear Regression RMSE: 133.8354\n",
      "Baseline Decision Tree (max_depth=6) RMSE: 158.5538\n",
      "Best Baseline Model: Linear Regression (RMSE: 133.8354)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. Part A: Baseline Models ---\n",
    "print(f\"\\n--- Part A: Baseline Models ---\")\n",
    "\n",
    "# Baseline 1: Linear Regression\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "preds_lr = pipeline_lr.predict(X_test)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, preds_lr))\n",
    "print(f\"Baseline Linear Regression RMSE: {rmse_lr:.4f}\")\n",
    "\n",
    "# Baseline 2: Decision Tree Regressor\n",
    "# max_depth is 6 per assignment spec \n",
    "pipeline_dt = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', DecisionTreeRegressor(max_depth=6, random_state=42))\n",
    "])\n",
    "pipeline_dt.fit(X_train, y_train)\n",
    "preds_dt = pipeline_dt.predict(X_test)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, preds_dt))\n",
    "print(f\"Baseline Decision Tree (max_depth=6) RMSE: {rmse_dt:.4f}\")\n",
    "\n",
    "if rmse_dt < rmse_lr:\n",
    "    best_baseline_rmse = rmse_dt\n",
    "    print(f\"Best Baseline Model: Decision Tree (RMSE: {best_baseline_rmse:.4f})\")\n",
    "else:\n",
    "    best_baseline_rmse = rmse_lr\n",
    "    print(f\"Best Baseline Model: Linear Regression (RMSE: {best_baseline_rmse:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb43155",
   "metadata": {},
   "source": [
    "# Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ff9576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part B: Ensemble Models ---\n",
      "Bagging Regressor (using max_depth=6 trees) RMSE: 154.9973\n",
      "Gradient Boosting (Tuned) Regressor RMSE: 82.3893\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Part B: Bagging and Boosting ---\n",
    "print(f\"\\n--- Part B: Ensemble Models ---\")\n",
    "\n",
    "# Bagging Regressor\n",
    "# CORRECTED: estimator now uses the baseline DT (max_depth=6) \n",
    "pipeline_bagging = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(max_depth=6, random_state=42), \n",
    "        n_estimators= 250, \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "pipeline_bagging.fit(X_train, y_train)\n",
    "preds_bagging = pipeline_bagging.predict(X_test)\n",
    "rmse_bagging = np.sqrt(mean_squared_error(y_test, preds_bagging))\n",
    "print(f\"Bagging Regressor (using max_depth=6 trees) RMSE: {rmse_bagging:.4f}\")\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "# TUNED: Improved hyperparameters for a stronger model\n",
    "pipeline_boosting = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(\n",
    "        n_estimators=300, \n",
    "        learning_rate=0.05, \n",
    "        max_depth=7, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "pipeline_boosting.fit(X_train, y_train)\n",
    "preds_boosting = pipeline_boosting.predict(X_test)\n",
    "rmse_boosting = np.sqrt(mean_squared_error(y_test, preds_boosting))\n",
    "print(f\"Gradient Boosting (Tuned) Regressor RMSE: {rmse_boosting:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c31bf",
   "metadata": {},
   "source": [
    ">>Bagging vs. Single Decision Tree (Variance Reduction)\n",
    "\n",
    "__Baseline Decision Tree (max_depth=6) RMSE: 158__\n",
    "\n",
    "__Bagging Regressor (with max_depth=6 trees) RMSE: 154__\n",
    "\n",
    "\n",
    "The bagging technique was not effective in this scenario. The RMSE improved by a negligible amount (158 â†’ 154).\n",
    "\n",
    "This result perfectly demonstrates the core concept of bagging: it is a variance-reduction technique.  It works by averaging the predictions of many diverse models, which is excellent for smoothing out high-variance models (like deep, overfit decision trees).\n",
    "\n",
    "However, the assignment required a max_depth=6 tree, which, for this complex dataset, is a high-bias (underfit) model. It's too simple to capture the data's patterns. Bagging a collection of models that are all systematically wrong in the same way (i.e., they all have high bias) does not fix the underlying bias. We simply get an average of many poor predictions. \n",
    "\n",
    "__It is highly likely that the bagging models performance would improve if we used overfit descisison trees (depth 10-15 )  as the base models for the bagging classifier__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4206bdf",
   "metadata": {},
   "source": [
    ">> Boosting vs. Baseline & Bagging (Bias Reduction)\n",
    "\n",
    "__Gradient Boosting Regressor RMSE: 82.3893__\n",
    "\n",
    "__Best Baseline (Linear Regression) RMSE: 133.8354__\n",
    "\n",
    "Bagging Regressor RMSE: 155.4459\n",
    "\n",
    "Thus, the GradientBoostingRegressor (RMSE: 82.39) achieved a dramatically better result than both the best single model (Linear Regression, RMSE: 133.84) and the Bagging ensemble (RMSE: 155.45).\n",
    "\n",
    "This strongly supports the hypothesis that boosting targets bias reduction. While Bagging trains models independently, Boosting is a sequential process. Each new tree is explicitly trained to correct the errors (the residuals or bias) of the previous trees. It effectively turns a collection of \"weak learners\" (like the shallow trees) into a single, powerful \"strong learner\" by systematically attacking the model's bias. This is why it performed so well on this high-bias problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f163eb2",
   "metadata": {},
   "source": [
    "# Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b4598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part C: Stacking Model ---\n",
      "Stacking Regressor RMSE: 83.0934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 6. Part C: Stacking ---\n",
    "print(f\"\\n--- Part C: Stacking Model ---\")\n",
    "\n",
    "# Define the Level-0 (Base) Learners\n",
    "# We must create a separate pipeline for KNN to include preprocessing\n",
    "pipeline_knn = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', KNeighborsRegressor(n_neighbors= 10)) # Using k=10 as a default\n",
    "])\n",
    "\n",
    "# Note: The Bagging and Boosting pipelines already include preprocessing.\n",
    "# When stacking pipelines, StackingRegressor will call fit/predict on them,\n",
    "# so the base learners *are* the full pipelines.\n",
    "base_learners = [\n",
    "    ('knn', pipeline_knn),\n",
    "    ('bagging', pipeline_bagging),\n",
    "    ('boosting', pipeline_boosting)\n",
    "]\n",
    "\n",
    "# Define the Level-1 (Meta) Learner\n",
    "meta_learner = Ridge()\n",
    "\n",
    "# Create the Stacking Regressor\n",
    "# Because the base learners are now pipelines,I  didn't  put the stacking regressor inside another pipeline & \n",
    "# set passthrough=True so the meta-learner gets the predictions from the base pipelines directly.\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True \n",
    ")\n",
    "\n",
    "# Train and evaluate Stacking\n",
    "# fit the StackingRegressor directly on the raw X_train, y_train\n",
    "# It will handle preprocessing internally via its base-learner pipelines\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "preds_stacking = stacking_regressor.predict(X_test)\n",
    "rmse_stacking = np.sqrt(mean_squared_error(y_test, preds_stacking))\n",
    "print(f\"Stacking Regressor RMSE: {rmse_stacking:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5ea11",
   "metadata": {},
   "source": [
    ">> Principle of Stacking \n",
    "\n",
    "Stacking (or Stacked Generalization) is a two-level ensemble method designed to combine the strengths of multiple diverse models. \n",
    "\n",
    "\n",
    "- Level 0 (Base Learners): A set of different models (e.g., KNN, Bagging, Boosting, as required by the assignment PS) are trained on the main training dataset\n",
    "\n",
    "- Level 1 (Meta-Learner): Instead of using the original data, a new model (the \"meta-learner,\" in this case, Ridge Regression ) is trained. Its input features are the predictions generated by the Level 0 models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b73526",
   "metadata": {},
   "source": [
    "# Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cd370",
   "metadata": {},
   "source": [
    "The meta-learner's job is to learn the optimal way to combine the base learners' predictions. It learns from the data which models to \"trust\" in which situations. For example, it might learn that the Boosting model is highly accurate but tends to over-predict at peak hours, and it can use the KNN model's prediction to help correct for that. By learning the strengths, weaknesses, and correlations between its base learners, the meta-learner creates a final prediction that is (ideally) more accurate than any single one of its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d4445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part D: Final Results ---\n",
      "Model RMSE Comparison:\n",
      "1. Baseline (Linear Regression): 133.8354\n",
      "2. Baseline (Decision Tree):      158.5538\n",
      "3. Bagging Regressor:             154.9973\n",
      "4. Gradient Boosting Regressor:   82.3893\n",
      "5. Stacking Regressor:            83.0934\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Part D: Final Analysis (Store results) ---\n",
    "print( \"\\n--- Part D: Final Results ---\")\n",
    "print(\"Model RMSE Comparison:\")\n",
    "print(f\"1. Baseline (Linear Regression): {rmse_lr:.4f}\" )\n",
    "print(f\"2. Baseline (Decision Tree):      {rmse_dt:.4f}\")\n",
    "print(f\"3. Bagging Regressor:             {rmse_bagging:.4f}\")\n",
    "print(f\"4. Gradient Boosting Regressor:   {rmse_boosting:.4f}\")\n",
    "print(f\"5. Stacking Regressor:            {rmse_stacking:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved RMSE results to 'model_rmse_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Save results to CSV ---\n",
    "try:\n",
    "    results = {\n",
    "        \"model\": [\n",
    "            \"Baseline (Linear Regression)\",\n",
    "            \"Baseline (Decision Tree)\",\n",
    "            \"Bagging Regressor\",\n",
    "            \"Gradient Boosting Regressor\",\n",
    "            \"Stacking Regressor\",\n",
    "        ],\n",
    "        \"rmse\": [rmse_lr, rmse_dt, rmse_bagging, rmse_boosting, rmse_stacking],\n",
    "    }\n",
    "    df_results = pd.DataFrame(results)\n",
    "    # round RMSE to 4 decimals for readability\n",
    "    df_results[\"rmse\"] = df_results[\"rmse\"].map(lambda x: round(float(x), 4))\n",
    "\n",
    "    output_path = \"model_rmse_results.csv\"\n",
    "    df_results.to_csv(output_path, index=False)\n",
    "    print(f\"Saved RMSE results to '{output_path}'\")\n",
    "except NameError as e:\n",
    "    print(\"Could not save RMSE results: some RMSE variable is not defined:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e29b10",
   "metadata": {},
   "source": [
    ">> Best Model and Final Conclusion\n",
    "\n",
    "__Best-Performing Model: The Gradient Boosting Regressor was the best-performing model, with the lowest RMSE of 82.3893. The Stacking Regressor (RMSE: 83.09) performed almost identically__\n",
    "\n",
    "Explanation (Bias-Variance & Diversity):\n",
    "\n",
    "The best ensemble (Gradient Boosting) massively outperformed the single model baseline (Linear Regression, RMSE: 133.84). This success is a clear lesson in the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947922e",
   "metadata": {},
   "source": [
    "1. __The Problem was High Bias__: The baseline models (Linear Regression and the shallow max_depth=6 Decision Tree) were both high-bias models. They were too simple to capture the complex, non-linear relationships in the hourly bike data. This is proven by their very high RMSE.\n",
    "\n",
    "2. __Boosting Solved the Bias__: The GradientBoostingRegressor won because it is a bias-reduction technique. It directly and aggressively corrected the errors of the weak baseline models, resulting in a low-bias, low-variance final model.\n",
    "\n",
    "3. __Why Stacking Didn't Win__: The Stacking model's performance was almost identical to the Boosting model's because its strongest component was the Boosting model. Its other two base learners (KNN and the high-bias Bagging model) were providing much weaker signals. The Ridge meta-learner likely learned that the safest and most accurate bet was to simply \"trust\" the Gradient Boosting model's prediction almost exclusively. It couldn't find a better combination, so it essentially just reproduced the result of its best member."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
